import os
import hashlib
import glob
from typing import List, Optional
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import HuggingFaceEmbeddings
except ImportError:
    # Fallback cho phiÃªn báº£n cÅ©
    from langchain.document_loaders import PyPDFLoader
    from langchain.vectorstores import FAISS
    from langchain.embeddings import HuggingFaceEmbeddings

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Cáº¥u hÃ¬nh thÆ° má»¥c
DOCUMENTS_DIR = "documents"
VECTOR_STORES_DIR = "vector_stores"


def create_directories():
    """Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t"""
    os.makedirs(DOCUMENTS_DIR, exist_ok=True)
    os.makedirs(VECTOR_STORES_DIR, exist_ok=True)


def get_file_hash(file_path: str) -> str:
    """Táº¡o hash Ä‘á»ƒ identify file duy nháº¥t"""
    with open(file_path, 'rb') as f:
        file_hash = hashlib.md5(f.read()).hexdigest()
    return file_hash[:8]  # Láº¥y 8 kÃ½ tá»± Ä‘áº§u


def get_available_documents() -> List[dict]:
    """Láº¥y danh sÃ¡ch táº¥t cáº£ documents cÃ³ sáºµn"""
    create_directories()
    documents = []

    pdf_files = glob.glob(os.path.join(DOCUMENTS_DIR, "*.pdf"))
    for pdf_file in pdf_files:
        filename = os.path.basename(pdf_file)
        file_size = os.path.getsize(pdf_file) / (1024 * 1024)  # MB
        file_hash = get_file_hash(pdf_file)

        # Kiá»ƒm tra xem cÃ³ vector store khÃ´ng
        vector_store_path = os.path.join(
            VECTOR_STORES_DIR, f"{filename}_{file_hash}")
        has_vector_store = os.path.exists(vector_store_path)

        documents.append({
            'filename': filename,
            'path': pdf_file,
            'size_mb': round(file_size, 2),
            'hash': file_hash,
            'vector_store_path': vector_store_path,
            'has_vector_store': has_vector_store
        })

    return documents


def process_single_document(pdf_path: str) -> Optional[FAISS]:
    """
    Xá»­ lÃ½ má»™t document duy nháº¥t
    """
    try:
        print(f"ğŸ“„ Äang xá»­ lÃ½: {os.path.basename(pdf_path)}")

        # Load PDF
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        if not documents:
            raise ValueError(f"KhÃ´ng thá»ƒ Ä‘á»c ná»™i dung tá»« file: {pdf_path}")

        # Enhanced text splitting vá»›i metadata
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Giáº£m chunk size Ä‘á»ƒ tÄƒng precision
            chunk_overlap=150,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""],
            length_function=len,
        )
        docs = text_splitter.split_documents(documents)

        # ThÃªm metadata cho tá»«ng chunk
        enhanced_docs = []
        filename = os.path.basename(pdf_path)

        for i, doc in enumerate(docs):
            # Filter out empty or meaningless chunks
            if len(doc.page_content.strip()) < 50:
                continue

            # Enhance metadata
            doc.metadata.update({
                'source_file': filename,
                'chunk_id': i,
                'chunk_length': len(doc.page_content),
                'total_chunks': len(docs)
            })
            enhanced_docs.append(doc)

        print(
            f"ğŸ“ Táº¡o {len(enhanced_docs)} chunks cháº¥t lÆ°á»£ng tá»« {len(docs)} chunks gá»‘c")

        # Táº¡o embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )

        # Táº¡o vector store
        vector_store = FAISS.from_documents(enhanced_docs, embeddings)

        # LÆ°u vá»›i tÃªn unique
        file_hash = get_file_hash(pdf_path)
        vector_store_path = os.path.join(
            VECTOR_STORES_DIR, f"{filename}_{file_hash}")
        vector_store.save_local(vector_store_path)

        print(
            f"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng {len(documents)} trang, táº¡o {len(enhanced_docs)} chunks")
        print(f"ğŸ’¾ LÆ°u vector store táº¡i: {vector_store_path}")

        return vector_store

    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ tÃ i liá»‡u {pdf_path}: {e}")
        return None


def load_vector_store(vector_store_path: str) -> Optional[FAISS]:
    """Load má»™t vector store cá»¥ thá»ƒ"""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        vector_store = FAISS.load_local(
            vector_store_path, embeddings, allow_dangerous_deserialization=True)
        print(f"âœ… ÄÃ£ load vector store tá»«: {vector_store_path}")
        return vector_store
    except Exception as e:
        print(f"âš ï¸ KhÃ´ng thá»ƒ load vector store {vector_store_path}: {e}")
        return None


def combine_vector_stores(documents: List[dict]) -> Optional[FAISS]:
    """Káº¿t há»£p nhiá»u vector stores thÃ nh má»™t"""
    try:
        combined_store = None
        total_docs = 0

        for doc_info in documents:
            if doc_info['has_vector_store']:
                store = load_vector_store(doc_info['vector_store_path'])
                if store:
                    if combined_store is None:
                        combined_store = store
                    else:
                        combined_store.merge_from(store)
                    total_docs += 1
                    print(
                        f"ğŸ“š ÄÃ£ thÃªm {doc_info['filename']} vÃ o combined store")

        if combined_store:
            print(
                f"âœ… ÄÃ£ káº¿t há»£p {total_docs} documents thÃ nh combined vector store")
            return combined_store
        else:
            print("âŒ KhÃ´ng cÃ³ vector store nÃ o Ä‘á»ƒ káº¿t há»£p")
            return None

    except Exception as e:
        print(f"âŒ Lá»—i káº¿t há»£p vector stores: {e}")
        return None


def process_all_documents() -> Optional[FAISS]:
    """Xá»­ lÃ½ táº¥t cáº£ documents trong thÆ° má»¥c"""
    documents = get_available_documents()

    if not documents:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y document nÃ o trong thÆ° má»¥c documents/")
        return None

    print(f"ğŸ“‹ TÃ¬m tháº¥y {len(documents)} documents:")
    for doc in documents:
        status = "âœ… Processed" if doc['has_vector_store'] else "âš ï¸ Not processed"
        print(f"  - {doc['filename']} ({doc['size_mb']} MB) {status}")

    # Xá»­ lÃ½ cÃ¡c documents chÆ°a cÃ³ vector store
    for doc in documents:
        if not doc['has_vector_store']:
            print(f"\nğŸ”„ Xá»­ lÃ½ {doc['filename']}...")
            process_single_document(doc['path'])

    # Cáº­p nháº­t láº¡i danh sÃ¡ch
    documents = get_available_documents()

    # Káº¿t há»£p táº¥t cáº£ vector stores
    return combine_vector_stores(documents)


def get_default_vector_store() -> Optional[FAISS]:
    """Láº¥y vector store máº·c Ä‘á»‹nh (Æ°u tiÃªn combined, fallback single)"""
    create_directories()

    # Thá»­ load combined store trÆ°á»›c
    documents = get_available_documents()

    if len(documents) > 1:
        # Nhiá»u documents - táº¡o combined store
        print("ğŸ”„ CÃ³ nhiá»u documents, táº¡o combined vector store...")
        return process_all_documents()
    elif len(documents) == 1:
        # Má»™t document duy nháº¥t
        doc = documents[0]
        if doc['has_vector_store']:
            print(f"ğŸ“„ Sá»­ dá»¥ng vector store cá»§a {doc['filename']}")
            return load_vector_store(doc['vector_store_path'])
        else:
            print(f"ğŸ”„ Xá»­ lÃ½ document {doc['filename']}...")
            return process_single_document(doc['path'])
    else:
        # KhÃ´ng cÃ³ document nÃ o - check fallback
        fallback_path = os.path.join(VECTOR_STORES_DIR, "doan3_index")
        if os.path.exists(fallback_path):
            print("ğŸ“„ Sá»­ dá»¥ng vector store cÅ© (doan3)")
            return load_vector_store(fallback_path)
        else:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y document nÃ o")
            return None


# Khá»Ÿi táº¡o vector store máº·c Ä‘á»‹nh
print("ğŸš€ Äang khá»Ÿi táº¡o vector store system...")
vector_store = get_default_vector_store()

if vector_store is None:
    print("âŒ KhÃ´ng thá»ƒ khá»Ÿi táº¡o vector store")
else:
    print("âœ… Vector store system sáºµn sÃ ng")
